{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e2c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f06428e",
   "metadata": {},
   "source": [
    "#### 9.1. Load Feature-Selected Datasets\n",
    "\n",
    "We load the datasets from step 8, which contain only the top features selected based on importance analysis.  \n",
    "At this point, the datasets are reduced in dimensionality and ready for scaling and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31419c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(\"./data/8/df_train.parquet\")\n",
    "df_test = pd.read_parquet(\"./data/8/df_test.parquet\")\n",
    "df_val = pd.read_parquet(\"./data/8/df_val.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cfd9c9",
   "metadata": {},
   "source": [
    "#### 9.2. Separate Features and Target\n",
    "\n",
    "We split each dataset into:\n",
    "- **X**: the feature matrix containing predictors  \n",
    "- **y**: the target column `isFraud`  \n",
    "\n",
    "This separation is required before applying scaling and feeding the data into models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a9e7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = df_train.columns.drop(\"isFraud\")\n",
    "X_train, y_train = df_train[feature_cols], df_train[\"isFraud\"]\n",
    "X_val, y_val = df_val[feature_cols], df_val[\"isFraud\"]\n",
    "X_test, y_test = df_test[feature_cols], df_test[\"isFraud\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a389c045",
   "metadata": {},
   "source": [
    "#### 9.3. Scale Features Using RobustScaler\n",
    "\n",
    "We apply `RobustScaler` to normalize the features:\n",
    "- Robust to outliers, which is important for financial datasets with high-value transactions\n",
    "- Scales the data using median and interquartile range instead of mean and standard deviation\n",
    "\n",
    "We fit the scaler on the training set and apply it to validation and test sets to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "156b278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=feature_cols)\n",
    "X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=feature_cols)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=feature_cols)\n",
    "\n",
    "X_train[\"isFraud\"] = y_train\n",
    "X_test[\"isFraud\"] = y_test\n",
    "X_val[\"isFraud\"] = y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6139a650",
   "metadata": {},
   "source": [
    "#### 9.4. Add Target Column Back and Save Scaled Datasets\n",
    "\n",
    "After scaling, we add the `isFraud` column back to each dataset to maintain a complete DataFrame.  \n",
    "Finally, the scaled datasets are saved into `./data/9/` for consistent use in downstream modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eeed61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./data/9/\", exist_ok=True)\n",
    "\n",
    "X_train.to_parquet(\"./data/9/df_train.parquet\", index=False)\n",
    "X_val.to_parquet(\"./data/9/df_val.parquet\", index=False)\n",
    "X_test.to_parquet(\"./data/9/df_test.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
